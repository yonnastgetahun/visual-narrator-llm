import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import logging
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProperAdjectiveBenchmark:
    """Proper adjective density benchmark using our actual trained models"""
    
    def __init__(self):
        self.test_prompts = [
            "Describe this urban scene:",
            "This beautiful landscape shows",
            "Generate caption for historic building",
            "What do you see in this city",
            "A peaceful natural scene with",
            "Modern architecture featuring",
            "Ancient ruins showing",
            "Colorful street art depicting"
        ]
        
        self.adjective_list = [
            'beautiful', 'vibrant', 'colorful', 'massive', 'enormous', 'gigantic',
            'peaceful', 'serene', 'tranquil', 'calm', 'chaotic', 'busy', 'bustling',
            'lively', 'dramatic', 'intense', 'powerful', 'emotional', 'elegant',
            'sophisticated', 'refined', 'ancient', 'historic', 'traditional', 'classic',
            'modern', 'contemporary', 'innovative', 'natural', 'organic', 'rustic',
            'urban', 'metropolitan', 'cosmopolitan', 'stunning', 'magnificent',
            'spectacular', 'quiet', 'bright', 'vivid', 'brilliant', 'dark', 'shadowy',
            'mysterious', 'warm', 'inviting', 'cozy', 'comfortable', 'cold', 'stark'
        ]
    
    def find_trained_model(self):
        """Find our actual trained model"""
        possible_paths = [
            "phase6/phase6_2_focused_adjectives",
            "phase6/adjective_complete_model", 
            "phase6/adjective_dominance_full",
            "phase6/adjective_model_proper"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                logger.info(f"âœ… Found model at: {path}")
                return path
        
        logger.warning("âŒ No trained model found, using base model")
        return None
    
    def load_model(self):
        """Load the best available model"""
        model_path = self.find_trained_model()
        
        tokenizer = AutoTokenizer.from_pretrained("facebook/opt-2.7b")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "facebook/opt-2.7b",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        if model_path:
            try:
                model = PeftModel.from_pretrained(base_model, model_path)
                logger.info("âœ… Loaded fine-tuned model successfully")
                return model, tokenizer
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load fine-tuned model: {e}")
        
        logger.info("ğŸ”„ Using base OPT-2.7B model")
        return base_model, tokenizer
    
    def count_adjectives(self, text):
        """Count adjectives in generated text"""
        text_lower = text.lower()
        return sum(1 for adj in self.adjective_list if adj in text_lower)
    
    def run_benchmark(self):
        """Run the proper adjective density benchmark"""
        logger.info("ğŸ¯ PROPER ADJECTIVE DENSITY BENCHMARK")
        logger.info("ğŸ“Š Testing our Phase 6.2 breakthrough claims")
        
        model, tokenizer = self.load_model()
        model.eval()
        
        results = []
        total_adjectives = 0
        
        for i, prompt in enumerate(self.test_prompts):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=80,
                    do_sample=True,
                    temperature=0.8,
                    repetition_penalty=1.2,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            adj_count = self.count_adjectives(generated)
            total_adjectives += adj_count
            
            result = {
                "prompt": prompt,
                "generated": generated,
                "adjectives_count": adj_count,
                "adjectives_found": [adj for adj in self.adjective_list if adj in generated.lower()]
            }
            results.append(result)
            
            logger.info(f"   {i+1}. '{prompt}'")
            logger.info(f"      â†’ {adj_count} adjectives: {result['adjectives_found']}")
            logger.info(f"      Generated: {generated[:100]}...")
        
        avg_adjectives = total_adjectives / len(self.test_prompts)
        
        benchmark_result = {
            "model_used": "Visual-Narrator-Enhanced" if isinstance(model, PeftModel) else "OPT-2.7B-Base",
            "average_adjectives_per_description": avg_adjectives,
            "total_test_prompts": len(self.test_prompts),
            "total_adjectives_found": total_adjectives,
            "results": results
        }
        
        logger.info(f"ğŸ‰ BENCHMARK COMPLETE!")
        logger.info(f"ğŸ“Š AVERAGE ADJECTIVES: {avg_adjectives:.2f} per description")
        logger.info(f"ğŸ¯ TARGET: 5.0+ (Phase 6.2: 9.88)")
        
        # Save detailed results
        with open("phase6.3/benchmarks/proper_benchmark_results.json", "w") as f:
            json.dump(benchmark_result, f, indent=2)
        
        return benchmark_result

if __name__ == "__main__":
    import os
    benchmark = ProperAdjectiveBenchmark()
    results = benchmark.run_benchmark()
    
    print("\n" + "="*50)
    if results["average_adjectives_per_description"] >= 5.0:
        print("ğŸ‰ PHASE 6.2 CLAIM VALIDATED!")
    else:
        print("ğŸ”„ Need to retrain to reach Phase 6.2 levels")
    print(f"ğŸ“ˆ Current: {results['average_adjectives_per_description']:.2f} adjectives/description")
    print(f"ğŸ¯ Target: 5.0+ (Phase 6.2: 9.88)")
    print("="*50)
