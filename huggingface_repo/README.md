---
language: en
license: apache-2.0
library_name: transformers
tags:
- visual-narrator
- image-captioning
- opt-2.7b
- lora
- computer-vision
- multimodal
pipeline_tag: text-generation
---

# Visual Narrator LLM - OPT-2.7B

A fine-tuned 2.7B parameter language model specialized in generating detailed, coherent image descriptions using LoRA efficient fine-tuning.

## Model Description

- **Architecture:** OPT-2.7B + LoRA fine-tuning
- **Training Data:** 555 specialized examples (urban scenes, people activities, nature)
- **Training Method:** Low-Rank Adaptation (LoRA) with r=16
- **Purpose:** Professional image description and visual narration
- **Performance:** 88% success rate on targeted scenes, 57.7% SOTA benchmark score

## Key Features

- üéØ **Specialized Training:** Focused on urban scenes (75% success) and people activities (100% success)
- ‚ö° **Efficient:** Only 0.30% parameters trained via LoRA
- üèóÔ∏è **Robust:** Proven training pipeline with comprehensive evaluation
- üìä **Benchmarked:** Competitive performance against larger models

## Usage

```python
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model
model_path = "your-username/visual-narrator-opt-2.7b"
tokenizer = AutoTokenizer.from_pretrained(model_path)
base_model = AutoModelForCausalLM.from_pretrained("facebook/opt-2.7b")
model = PeftModel.from_pretrained(base_model, model_path)

# Generate descriptions
def describe_image(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
    inputs = {k: v.cuda() for k, v in inputs.items()}
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=80,
        temperature=0.4,
        do_sample=True,
        repetition_penalty=1.8,
        no_repeat_ngram_size=3
    )
    
    description = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return description.replace(prompt, "").strip()

# Example usage
description = describe_image("Describe this image: a beautiful sunset over mountains")
print(description)
Training Details
Phase 4 & 5 Achievements:
Phase 4.1: Successful 2.7B model foundation

Phase 4.2: Professional dataset integration

Phase 4.3: Advanced training pipeline

Phase 4.4: Comprehensive benchmarking

Phase 5: SOTA-targeted optimizations

Performance Metrics:
Overall Success Rate: 88%

SOTA Benchmark Score: 57.7%

Urban Scene Performance: 75%

People Scene Performance: 100%

Training Efficiency: 0.30% parameters trained

Files Included
pytorch_model.bin - LoRA adapter weights

adapter_config.json - LoRA configuration

special_tokens_map.json - Tokenizer mappings

tokenizer_config.json - Tokenizer settings

README.md - This model card

training_phases/ - Complete documentation

Citation
If you use this model in your research, please cite:@software{visual_narrator_2024,
  title = {Visual Narrator LLM: OPT-2.7B Fine-tuned for Image Description},
  author = {Your Name},
  year = {2024},
  url = {https://huggingface.co/your-username/visual-narrator-opt-2.7b}
}License
Apache 2.0
