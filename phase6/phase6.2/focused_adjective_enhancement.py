import torch
import logging
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def focused_adjective_training():
    """Focused training specifically for adjective generation"""
    
    logger.info("ðŸŽ¯ PHASE 6.2: FOCUSED ADJECTIVE ENHANCEMENT")
    
    # Load the successful Phase 6.1 model
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel, LoraConfig, get_peft_model
    
    model_id = "facebook/opt-2.7b"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # Load the proven Phase 6.1 model
    try:
        model = PeftModel.from_pretrained(base_model, "adjective_complete_model")
        logger.info("âœ… Loaded successful Phase 6.1 adjective model!")
    except:
        logger.info("ðŸ”„ Using base model")
        model = base_model
    
    # FOCUSED adjective training data - force adjective usage
    focused_examples = [
        "Describe this image: beautiful vibrant colorful scene",
        "This shows massive enormous gigantic structure",
        "A peaceful serene tranquil calm landscape",
        "Chaotic busy bustling lively city street",
        "Dramatic intense powerful emotional moment",
        "Elegant sophisticated refined architecture",
        "Ancient historic traditional classic building",
        "Modern contemporary innovative design",
        "Natural organic rustic environment",
        "Urban metropolitan cosmopolitan cityscape",
        "Stunning magnificent spectacular view",
        "Quiet peaceful serene atmosphere",
        "Bright vivid brilliant colors",
        "Dark shadowy mysterious scene",
        "Warm inviting cozy comfortable space",
        "Cold stark minimalist modern",
        "Luxurious opulent extravagant details",
        "Simple minimal clean aesthetic",
        "Complex intricate detailed patterns",
        "Expansive vast wide open space"
    ] * 5  # Repeat for more training
    
    logger.info(f"ðŸŽ¯ Using {len(focused_examples)} FOCUSED adjective examples")
    
    # Enhanced LoRA for adjective focus
    lora_config = LoraConfig(
        r=32,  # Optimal for adjective learning
        lora_alpha=64,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.01,  # Very low dropout for focused learning
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # Manual training with adjective-focused prompts
    from transformers import AdamW, get_linear_schedule_with_warmup
    
    optimizer = AdamW(model.parameters(), lr=2e-5)  # Higher LR for focused training
    
    # Tokenize with emphasis on adjective patterns
    encoded_data = []
    for example in focused_examples:
        encoded = tokenizer(example, truncation=True, max_length=64, padding='max_length')
        encoded_data.append(encoded)
    
    logger.info("ðŸ”¥ Starting FOCUSED adjective training...")
    
    model.train()
    num_epochs = 3
    batch_size = 4
    
    for epoch in range(num_epochs):
        total_loss = 0
        batches = 0
        
        for i in range(0, len(encoded_data), batch_size):
            batch = encoded_data[i:i+batch_size]
            
            input_ids = torch.tensor([item['input_ids'] for item in batch]).to(model.device)
            attention_mask = torch.tensor([item['attention_mask'] for item in batch]).to(model.device)
            labels = input_ids.clone()
            
            # Forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()
            batches += 1
            
            # Backward pass
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            if batches % 10 == 0:
                logger.info(f"ðŸ“ˆ Epoch {epoch+1}, Batch {batches}: Loss {loss.item():.4f}")
        
        avg_loss = total_loss / batches
        logger.info(f"ðŸŽ¯ Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}")
    
    # Save the enhanced model
    model.save_pretrained("./phase6_2_focused_adjectives")
    logger.info("âœ… Focused adjective training complete!")
    
    # Comprehensive adjective testing
    logger.info("ðŸ§ª COMPREHENSIVE ADJECTIVE TESTING...")
    
    test_prompts = [
        "Describe this urban scene:",
        "This beautiful landscape shows",
        "Generate caption for historic building",
        "What do you see in this city",
        "A peaceful natural scene with",
        "Modern architecture featuring",
        "Ancient ruins showing",
        "Colorful street art depicting"
    ]
    
    model.eval()
    total_adjectives = 0
    total_tests = 0
    
    adjective_list = ['beautiful', 'vibrant', 'colorful', 'massive', 'enormous', 'gigantic', 'peaceful', 
                     'serene', 'tranquil', 'calm', 'chaotic', 'busy', 'bustling', 'lively', 'dramatic',
                     'intense', 'powerful', 'emotional', 'elegant', 'sophisticated', 'refined', 'ancient',
                     'historic', 'traditional', 'classic', 'modern', 'contemporary', 'innovative', 'natural',
                     'organic', 'rustic', 'urban', 'metropolitan', 'cosmopolitan', 'stunning', 'magnificent',
                     'spectacular', 'quiet', 'bright', 'vivid', 'brilliant', 'dark', 'shadowy', 'mysterious',
                     'warm', 'inviting', 'cozy', 'comfortable', 'cold', 'stark', 'luxurious', 'opulent',
                     'extravagant', 'simple', 'minimal', 'clean', 'complex', 'intricate', 'expansive', 'vast']
    
    for prompt in test_prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=80,
                do_sample=True,
                temperature=0.8,  # Higher temperature for more creative adjectives
                repetition_penalty=1.2,  # Higher penalty to avoid repetition
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Count adjectives in generated text
        adj_count = sum(1 for adj in adjective_list if adj in generated.lower())
        total_adjectives += adj_count
        total_tests += 1
        
        logger.info(f"ðŸŽ¯ PROMPT: '{prompt}'")
        logger.info(f"   GENERATED: {generated}")
        logger.info(f"   ADJECTIVES: {adj_count} detected")
        logger.info("   ---")
    
    avg_adjectives = total_adjectives / total_tests if total_tests > 0 else 0
    logger.info(f"ðŸ“Š FINAL RESULTS: {avg_adjectives:.2f} adjectives per description")
    logger.info(f"ðŸŽ¯ TARGET: 5.0+ adjectives per description")
    logger.info(f"ðŸ“ˆ IMPROVEMENT: From 4.15 to {avg_adjectives:.2f}")
    
    return model, tokenizer, avg_adjectives

if __name__ == "__main__":
    model, tokenizer, avg_adjectives = focused_adjective_training()
    
    if avg_adjectives >= 5.0:
        print("ðŸŽ‰ PHASE 6.2 BREAKTHROUGH ACHIEVED!")
        print(f"ðŸ“ˆ SUCCESS: {avg_adjectives:.2f} adjectives per description!")
    else:
        print("ðŸ”„ Additional training needed to reach 5.0+ target")
        print(f"ðŸ“Š Current: {avg_adjectives:.2f} adjectives per description")
