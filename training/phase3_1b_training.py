#!/usr/bin/env python3
"""
PHASE 3.1: Actual 1B Parameter Training
First real cloud training on A100
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import load_dataset
import time

print("ğŸš€ STARTING ACTUAL 1B PARAMETER TRAINING")
print("=" * 60)

# Use a real 1B+ parameter model
model_name = "microsoft/DialoGPT-large"  # 774M - good starting point
print(f"ğŸ¤– Loading: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Critical: Add padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Move to GPU
model = model.cuda()
print(f"âœ… Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters")

# Load larger dataset for real training
print("ğŸ“¥ Loading training data (1K examples)...")
dataset = load_dataset("conceptual_captions", split="train[:1000]")

# Format training data
texts = [f"Describe this image: {example['caption']}" for example in dataset]

# Tokenize
print("ğŸ”¤ Tokenizing dataset...")
encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors="pt")

# Create dataset with labels
class TrainingDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        item['labels'] = item['input_ids'].clone()
        return item
    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = TrainingDataset(encodings)
print(f"ğŸ“Š Training dataset: {len(train_dataset)} samples")

# Training arguments optimized for A100
training_args = TrainingArguments(
    output_dir="./outputs/phase3_1b",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=8,  # Larger batches on A100
    save_steps=500,
    logging_dir='./logs/phase3_1b',
    logging_steps=50,
    report_to=None,
    learning_rate=5e-5,
    fp16=True,  # Use mixed precision on A100
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

print("ğŸ¯ STARTING CLOUD TRAINING ON A100!")
print("ğŸ’¡ This is your first real training on cloud GPU!")
print("â° Estimated time: 2-5 minutes")

# Start training!
trainer.train()

print("ğŸ‰ PHASE 3.1 TRAINING COMPLETED!")
print("ğŸ’¾ Saving model...")
trainer.save_model()
tokenizer.save_pretrained("./outputs/phase3_1b")

print("ğŸš€ PHASE 3.1 SUCCESS! Ready for Phase 3.2 scaling!")
