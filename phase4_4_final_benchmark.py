#!/usr/bin/env python3
"""
PHASE 4.4: FINAL CLEAN BENCHMARKING
Comprehensive evaluation of our improved Visual Narrator
"""

print("üöÄ PHASE 4.4: FINAL CLEAN BENCHMARKING")
print("=" * 50)

import torch
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import pandas as pd
import time

class ImprovedVisualNarrator:
    def __init__(self, model_path="./phase4/fixed_model"):
        """Initialize the improved model"""
        print("ü§ñ Loading improved Visual Narrator...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        base_model = AutoModelForCausalLM.from_pretrained(
            "facebook/opt-2.7b",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        self.model = PeftModel.from_pretrained(base_model, model_path)
        self.model.eval()
        print("‚úÖ Improved model loaded!")
    
    def clean_generation(self, prompt, max_length=60):
        """Clean generation with artifact filtering"""
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_length,
            num_return_sequences=1,
            temperature=0.4,
            do_sample=True,
            repetition_penalty=2.0,
            no_repeat_ngram_size=3,
            early_stopping=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        raw_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        clean_output = raw_output.replace(prompt, "").strip()
        
        # Filter out common artifacts
        artifacts = [
            "strutConnector", "guiActive", "guiIcon", "attRot", "madeupword",
            "TheNitrome", "SolidGoldMagikarp", "Downloadha", "externalToEVAOnly",
            "guiName", "srfAttach", "PsyNetMessage", "DragonMagazine", "UCHIJ",
            "embedreportprint", "clone", "0001", "0002", "Ë£è", "ÔøΩ", "<mask>",
            "<|endoftext|>", "RandomRedditorWithNo"
        ]
        
        for artifact in artifacts:
            clean_output = clean_output.replace(artifact, "")
        
        # Clean up extra spaces and punctuation
        import re
        clean_output = re.sub(r'\s+', ' ', clean_output).strip()
        clean_output = re.sub(r'[.,!?;]*$', '', clean_output)
        
        return clean_output if clean_output else raw_output.replace(prompt, "").strip()

def run_comprehensive_benchmark():
    """Run comprehensive benchmark on improved model"""
    print("\nüéØ Running Comprehensive Benchmark...")
    
    # Initialize improved model
    narrator = ImprovedVisualNarrator()
    
    # Test scenarios for comprehensive evaluation
    test_scenarios = [
        {
            "category": "Nature",
            "prompts": [
                "Describe this image: a beautiful sunset over mountains",
                "What do you see in this picture: a peaceful forest with sunlight",
                "Generate a caption: a tropical beach with palm trees",
                "This image shows a field of wildflowers"
            ]
        },
        {
            "category": "Urban", 
            "prompts": [
                "Describe this image: a busy city street with skyscrapers",
                "What do you see here: a modern kitchen with appliances",
                "Generate a caption: a historic building with architecture",
                "This picture shows a subway station with commuters"
            ]
        },
        {
            "category": "People",
            "prompts": [
                "Describe this image: a sports event with athletes",
                "What do you see in this photo: a classroom with students",
                "Generate a caption: a concert with musicians performing",
                "This image shows a family gathering in a living room"
            ]
        }
    ]
    
    results = []
    total_tests = 0
    successful_generations = 0
    
    print("\nüìù Generating Descriptions...")
    for scenario in test_scenarios:
        print(f"\nüåÑ {scenario['category']} Scenarios:")
        for prompt in scenario["prompts"]:
            total_tests += 1
            
            try:
                description = narrator.clean_generation(prompt)
                
                # Quality assessment
                is_coherent = len(description) > 10 and '.' in description
                is_relevant = any(word in description.lower() for word in ['sunset', 'forest', 'beach', 'city', 'street', 'kitchen', 'building', 'sports', 'classroom', 'concert', 'family'])
                has_artifacts = any(artifact in description for artifact in ["strutConnector", "guiActive", "0001", "0002", "ÔøΩ"])
                
                if is_coherent and not has_artifacts:
                    successful_generations += 1
                    status = "‚úÖ SUCCESS"
                elif is_coherent:
                    successful_generations += 0.5  # Partial success
                    status = "‚ö†Ô∏è  MINOR ARTIFACTS"
                else:
                    status = "‚ùå NEEDS WORK"
                
                results.append({
                    'category': scenario['category'],
                    'prompt': prompt,
                    'description': description,
                    'coherent': is_coherent,
                    'relevant': is_relevant,
                    'artifacts': has_artifacts,
                    'status': status
                })
                
                print(f"   {status}: {prompt[:35]}...")
                if description:
                    print(f"      üìñ {description[:60]}...")
                
            except Exception as e:
                print(f"   ‚ùå ERROR: {prompt[:35]}... -> {e}")
                results.append({
                    'category': scenario['category'],
                    'prompt': prompt,
                    'description': f"ERROR: {e}",
                    'coherent': False,
                    'relevant': False,
                    'artifacts': False,
                    'status': "‚ùå ERROR"
                })
    
    # Performance benchmarking
    print("\n‚ö° Performance Benchmarking...")
    performance_metrics = {}
    
    # Inference speed
    test_prompt = "Describe this image: a beautiful landscape"
    start_time = time.time()
    
    for i in range(5):
        narrator.clean_generation(test_prompt)
    
    end_time = time.time()
    performance_metrics['avg_inference_time'] = (end_time - start_time) / 5
    
    # Memory usage
    if torch.cuda.is_available():
        performance_metrics['gpu_memory_gb'] = torch.cuda.memory_allocated() / 1024**3
    
    print(f"   ‚úÖ Average inference time: {performance_metrics['avg_inference_time']:.2f}s")
    if 'gpu_memory_gb' in performance_metrics:
        print(f"   ‚úÖ GPU memory usage: {performance_metrics['gpu_memory_gb']:.2f}GB")
    
    # Calculate overall scores
    coherence_rate = sum(1 for r in results if r['coherent']) / len(results) * 100
    relevance_rate = sum(1 for r in results if r['relevant']) / len(results) * 100
    artifact_rate = sum(1 for r in results if r['artifacts']) / len(results) * 100
    success_rate = (successful_generations / total_tests) * 100
    
    # Generate final report
    final_report = {
        'summary': {
            'total_tests': total_tests,
            'successful_generations': successful_generations,
            'success_rate': success_rate,
            'coherence_rate': coherence_rate,
            'relevance_rate': relevance_rate,
            'artifact_rate': artifact_rate
        },
        'performance_metrics': performance_metrics,
        'category_breakdown': {},
        'detailed_results': results,
        'model_info': {
            'base_model': 'facebook/opt-2.7b',
            'fine_tuning': 'LoRA (conservative)',
            'training_data': '416 diverse examples',
            'training_strategy': 'Early stopping with validation'
        },
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    # Category breakdown
    for scenario in test_scenarios:
        category_results = [r for r in results if r['category'] == scenario['category']]
        if category_results:
            category_success = sum(1 for r in category_results if 'SUCCESS' in r['status']) / len(category_results) * 100
            final_report['category_breakdown'][scenario['category']] = {
                'tests': len(category_results),
                'success_rate': category_success
            }
    
    # Save report
    import os
    os.makedirs("./phase4/final_benchmark", exist_ok=True)
    
    with open("./phase4/final_benchmark/comprehensive_report.json", "w") as f:
        json.dump(final_report, f, indent=2)
    
    # Save readable summary
    with open("./phase4/final_benchmark/summary.txt", "w") as f:
        f.write("VISUAL NARRATOR LLM - FINAL BENCHMARK REPORT\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Overall Success Rate: {success_rate:.1f}%\n")
        f.write(f"Coherence Rate: {coherence_rate:.1f}%\n")
        f.write(f"Relevance Rate: {relevance_rate:.1f}%\n")
        f.write(f"Artifact Rate: {artifact_rate:.1f}%\n")
        f.write(f"Inference Time: {performance_metrics['avg_inference_time']:.2f}s\n\n")
        
        f.write("CATEGORY BREAKDOWN:\n")
        for category, stats in final_report['category_breakdown'].items():
            f.write(f"  {category}: {stats['success_rate']:.1f}% success\n")
        
        f.write("\nSAMPLE GENERATIONS:\n")
        for i, result in enumerate(results[:8]):  # First 8 samples
            f.write(f"\n{i+1}. {result['prompt'][:40]}...\n")
            f.write(f"   ‚Üí {result['description'][:80]}...\n")
            f.write(f"   [{result['status']}]\n")
    
    return final_report

def main():
    """Main benchmarking pipeline"""
    try:
        print("üöÄ Starting Final Phase 4.4 Benchmarking...")
        
        # Run comprehensive benchmark
        report = run_comprehensive_benchmark()
        
        # Print final summary
        print("\nüéØ PHASE 4.4 FINAL BENCHMARK COMPLETE!")
        print("=" * 45)
        print(f"üìä Overall Success Rate: {report['summary']['success_rate']:.1f}%")
        print(f"üéØ Coherence: {report['summary']['coherence_rate']:.1f}%")
        print(f"üîó Relevance: {report['summary']['relevance_rate']:.1f}%")
        print(f"üõ†Ô∏è  Artifacts: {report['summary']['artifact_rate']:.1f}%")
        print(f"‚ö° Inference: {report['performance_metrics']['avg_inference_time']:.2f}s")
        
        print("\nüìà CATEGORY PERFORMANCE:")
        for category, stats in report['category_breakdown'].items():
            print(f"   {category}: {stats['success_rate']:.1f}%")
        
        print(f"\nüíæ Reports saved to: ./phase4/final_benchmark/")
        print("üöÄ PHASE 4 COMPLETED SUCCESSFULLY!")
        
    except Exception as e:
        print(f"‚ùå Benchmarking error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
