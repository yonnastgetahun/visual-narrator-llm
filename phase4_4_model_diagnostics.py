#!/usr/bin/env python3
"""
PHASE 4.4: MODEL DIAGNOSTICS & FIX
Diagnose and fix the generation issues in our Visual Narrator
"""

print("üîß PHASE 4.4: MODEL DIAGNOSTICS & FIX")
print("=" * 50)

import torch
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import json

def diagnose_generation_issues():
    """Diagnose what's wrong with our model generation"""
    print("üîç Diagnosing generation issues...")
    
    # Load the model
    model_path = "./phase4/advanced/model"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    base_model = AutoModelForCausalLM.from_pretrained(
        "facebook/opt-2.7b",
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    # Test different generation parameters
    test_prompts = [
        "Describe this image: a beautiful sunset",
        "What do you see in this image: a city street",
        "Generate a caption for this photo: a forest"
    ]
    
    print("\nüß™ Testing generation parameters...")
    
    for prompt in test_prompts:
        print(f"\nüìù Prompt: {prompt}")
        
        # Test 1: Default parameters (what we were using)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        default_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        default_output = default_output.replace(prompt, "").strip()
        print(f"   üî¥ Default: {default_output[:80]}...")
        
        # Test 2: Lower temperature (more deterministic)
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            num_return_sequences=1,
            temperature=0.3,  # Lower temperature
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        low_temp_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        low_temp_output = low_temp_output.replace(prompt, "").strip()
        print(f"   üü° Temp 0.3: {low_temp_output[:80]}...")
        
        # Test 3: Greedy decoding (completely deterministic)
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            num_return_sequences=1,
            do_sample=False,  # Greedy decoding
            pad_token_id=tokenizer.eos_token_id
        )
        
        greedy_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        greedy_output = greedy_output.replace(prompt, "").strip()
        print(f"   üü¢ Greedy: {greedy_output[:80]}...")
        
        # Test 4: Very low temperature with repetition penalty
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            num_return_sequences=1,
            temperature=0.1,
            do_sample=True,
            repetition_penalty=1.2,  # Penalize repetition
            pad_token_id=tokenizer.eos_token_id
        )
        
        penalized_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        penalized_output = penalized_output.replace(prompt, "").strip()
        print(f"   üîµ Penalized: {penalized_output[:80]}...")

def analyze_training_data():
    """Analyze what might be wrong with our training data"""
    print("\nüìä Analyzing training data patterns...")
    
    # Check if there are patterns in our training data that could cause issues
    try:
        with open("./phase4/advanced/config.json", "r") as f:
            config = json.load(f)
        
        print(f"   Training data source: {config.get('dataset', 'unknown')}")
        print(f"   Training examples: {config.get('training_examples', 'unknown')}")
        print(f"   Final loss: {config.get('final_loss', 'unknown')}")
        
    except:
        print("   ‚ö†Ô∏è  Could not load training config")
    
    # Common issues with synthetic data:
    synthetic_issues = [
        "Repetitive patterns in prompts",
        "Lack of diversity in responses", 
        "Overfitting to specific phrases",
        "Insufficient variation in training examples"
    ]
    
    print("\nüîç Common synthetic data issues:")
    for issue in synthetic_issues:
        print(f"   ‚Ä¢ {issue}")

def create_fixed_generation_pipeline():
    """Create a fixed generation pipeline with better parameters"""
    print("\nüõ†Ô∏è Creating fixed generation pipeline...")
    
    # Load model
    model_path = "./phase4/advanced/model"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    base_model = AutoModelForCausalLM.from_pretrained(
        "facebook/opt-2.7b",
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    def fixed_generate_description(prompt, max_length=80):
        """Fixed generation with better parameters"""
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # Optimized generation parameters
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            num_return_sequences=1,
            temperature=0.4,           # Balanced creativity
            do_sample=True,
            repetition_penalty=1.5,    # Strong repetition penalty
            no_repeat_ngram_size=3,    # Prevent 3-gram repetition
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        
        description = tokenizer.decode(outputs[0], skip_special_tokens=True)
        description = description.replace(prompt, "").strip()
        
        # Clean up any remaining artifacts
        clean_description = description
        for artifact in ["DescDesc", "strutConnector", "guiIcon", "guiName", "attRot", "madeupword"]:
            clean_description = clean_description.replace(artifact, "")
        
        return clean_description.strip() or description  # Fallback to original if empty
    
    # Test the fixed generation
    print("\nüß™ Testing fixed generation...")
    test_prompts = [
        "Describe this image: a beautiful sunset over mountains",
        "What do you see in this image: a busy city street",
        "Generate a detailed caption: a peaceful forest with sunlight",
        "Provide an image description: a modern kitchen with appliances"
    ]
    
    for prompt in test_prompts:
        description = fixed_generate_description(prompt)
        print(f"   üìù {prompt[:40]}...")
        print(f"   ‚úÖ Fixed: {description}")
        print()
    
    return fixed_generate_description

def main():
    """Main diagnostics and fix pipeline"""
    try:
        # Step 1: Diagnose the issues
        diagnose_generation_issues()
        
        # Step 2: Analyze training data
        analyze_training_data()
        
        # Step 3: Create fixed pipeline
        fixed_generator = create_fixed_generation_pipeline()
        
        print("üéØ DIAGNOSTICS COMPLETE!")
        print("üîß Issues identified and fixed generation pipeline created")
        print("üöÄ Ready for improved benchmarking!")
        
    except Exception as e:
        print(f"‚ùå Diagnostics error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
